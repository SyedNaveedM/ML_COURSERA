{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "##### Neural networks were built to mimic the functioning of the human brain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Traditional learning algorithms were not able to scale their performance with the amount of data available and reached a stagnation point. This is where neural networks come into play. They are able to learn from large amounts of data and generalize well on unseen data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For understanding the working of neural networks, we need to understand the basic building blocks of neural networks. Let us take a simple example of demand prediction for a product. Let x be the price of the product and y be the demand for the product. We can represent this relationship as y = f(x). The function f(x) is the neural network that we are trying to learn. Let us use logistic regression to classify the product as high demand or low demand. The logistic regression model can be represented as y = sigmoid(w*x + b). Here w and b are the weights and bias of the model. The sigmoid function is used to convert the output of the linear model to a probability value between 0 and 1. This is a single neuron neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now let us consider a more complex example where we have multiple features to predict the demand for the product. We can represent this as y = f(x1, x2, x3, ..., xn). The function f(x1, x2, x3, ..., xn) is the neural network that we are trying to learn. We can represent this as y = sigmoid(w1*x1 + w2*x2 + w3*x3 + ... + wn*xn + b). This is a multi-layer neural network with multiple neurons in the hidden layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A layer in a neural network is a collection of neurons that perform the same operation on the input data. The input layer is the first layer of the neural network that takes the input data. The hidden layers are the layers between the input and output layers. The output layer is the last layer of the neural network that produces the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Activation functions are used to introduce non-linearity in the neural network. The most commonly used activation functions are sigmoid, tanh, and ReLU. The sigmoid function is used to convert the output of the linear model to a probability value between 0 and 1. The tanh function is used to convert the output of the linear model to a value between -1 and 1. The ReLU function is used to introduce non-linearity in the neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ReLU is the most commonly used activation function in deep learning. It is defined as f(x) = max(0, x). ReLU is computationally efficient and does not suffer from the vanishing gradient problem. The vanishing gradient problem occurs when the gradient of the activation function becomes very small, leading to slow convergence of the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Each neuron in a particular layer has access to all the outputs of the previous layer and it eventually learns to ignore the less important features or activations. This is done by adjusting the weights and biases of the neurons during the training process. The weights and biases are updated using an optimization algorithm such as gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A neural network does the feature engineering which we had to do manually in traditional machine learning algorithms. It automatically learns the important features from the data and generalizes well on unseen data. This is the power of neural networks and deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Face recognition is done using neural networks. Basically any image is represented as a matrix of pixel values. The neural network learns the important features from the image and classifies it as a particular person. This is done by training the neural network on a large dataset of images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### By convention we use layer 0 for the input layer, layer 1 for the first hidden layer, layer 2 for the second hidden layer, and so on. The output layer is the last layer of the neural network. We use a superscript in square brackets to denote the layer number. For example, a[1] is the activation of the first hidden layer and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Activation value of layer l, unit neuron j is z[l][j] = g(w[l][j] . a[l-1] + b[l][j]) where g is the activation function. The activation value of layer l, unit neuron j is a[l][j] = g(z[l][j]). One example of g is the sigmoid function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward propogation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Forward propogation is used for making predictions from neural networks. It is the process of passing the input data through the neural network to get the output. The output of the neural network is the prediction made by the model. The forward propagation algorithm is as follows:\n",
    "\n",
    "##### 1. Initialize the input layer with the input data.\n",
    "##### 2. For each layer l from 1 to L:\n",
    "#####     a[l] = g(z[l])\n",
    "#####     z[l] = w[l] . a[l-1] + b[l]\n",
    "##### 3. The output of the neural network is the activation value of the output layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation of forward propogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense(W,b,a_in):\n",
    "    # The columns of W are the weights of the units\n",
    "    units=W.shape[1]\n",
    "    a_out=np.zeros(units)\n",
    "    for i in range(units):\n",
    "        input=W[:,i]*b[i]\n",
    "        z=np.dot(a_in,input)+b[i]\n",
    "        a_out[i]=1/(1+np.exp(-z))\n",
    "    return a_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### While compiling our model in tensorflow we need to specify the optimizer and the loss function. The optimizer is used to update the weights and biases of the neural network during training. The loss function is used to measure the error between the predicted output and the actual output. The loss function is minimized during training to improve the performance of the neural network. One of the examples of loss functions is binary cross entropy function which is used for binary classification problems. The binary cross entropy function is defined as L(y, y_hat) = -1/m * sum(y * log(y_hat) + (1-y) * log(1-y_hat)) where y is the actual output and y_hat is the predicted output. The binary cross entropy function measures the error between the predicted output and the actual output. The optimizer is used to minimize the loss function during training. One of the examples of optimizers is the Adam optimizer which is used to update the weights and biases of the neural network during training. The Adam optimizer is an extension of the stochastic gradient descent algorithm and is computationally efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tensorflow computes the partial derivatives in gradient descent using backpropogation. Backpropogation is the process of computing the gradients of the loss function with respect to the weights and biases of the neural network. The gradients are used to update the weights and biases of the neural network during training. The backpropagation algorithm is as follows:\n",
    "\n",
    "##### 1. Compute the error between the predicted output and the actual output.\n",
    "##### 2. Compute the gradients of the loss function with respect to the weights and biases of the neural network.\n",
    "##### 3. Update the weights and biases of the neural network using the gradients and the learning rate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ReLU-Rectified Linear Unit. It is basically max(0,x) where x is the input to the neuron. It is computationally efficient and does not suffer from the vanishing gradient problem. The vanishing gradient problem occurs when the gradient of the activation function becomes very small, leading to slow convergence of the neural network. It is the most commonly used activation function in deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the right activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### When the output is either a 0 or 1(binary classification problem) then the sigmoid function is used as the activation function. \n",
    "##### When the output is a value between -1 and 1 then the tanh function is used as the activation function.\n",
    "##### When the output is a value greater than 0 then the ReLU function is used as the activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In the hidden layers we mostly use only the ReLU function as the activation function. This is because the ReLU function is computationally efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For multiclass classfication problem we use softmax regression which is a generalization of logistic regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If there are n different possible outputs then the softmax function is used as the activation function. The softmax function is defined as a[i] = e^(z[i]) / sum(e^(z[j])) where i = 1 to n. The softmax function converts the output of the linear model to a probability value between 0 and 1. The output of the softmax function is a probability distribution over the n different possible outputs. The output with the highest probability is the predicted output of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cost function of softmax regression is the cross entropy loss function. The cross entropy loss function is defined as L(y, y_hat) = -1/m * sum(y * log(y_hat)) where y is the actual output and y_hat is the predicted output. The cross entropy loss function measures the error between the predicted output and the actual output. The cross entropy loss function is minimized during training to improve the performance of the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For using softmax regression we simply change the activation function of the output layer to softmax function and the loss function to cross entropy loss function. The rest of the neural network remains the same. Also, the number of output neurons is equal to the number of classes in the multiclass classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multi-label classification problems: If there can be multiple labels associated with a single input then it is a multi-label classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SparseCategorialCrossentropy or CategoricalCrossEntropy \n",
    "Tensorflow has two potential formats for target values and the selection of the loss defines which is expected.\n",
    "\n",
    "SparseCategorialCrossentropy: expects the target to be an integer corresponding to the index. For example, if there are 10 potential target values, y would be between 0 and 9.\n",
    "CategoricalCrossEntropy: Expects the target value of an example to be one-hot encoded where the value at the target index is 1 while the other N-1 entries are zero. An example with 10 potential target values, where the target is 2 would be [0,0,1,0,0,0,0,0,0,0]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directly using softmax regression in the output layer as the activation function is actually numerically unstable and there is a better method of implementing it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Not preferred way of implementing softmax regression\n",
    "##### model = Sequential(\n",
    "    [ \n",
    "        Dense(25, activation = 'relu'),\n",
    "        Dense(15, activation = 'relu'),\n",
    "        Dense(4, activation = 'softmax')    # < softmax activation here\n",
    "    ]\n",
    ")\n",
    "##### model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preferred way of implementing softmax regression\n",
    "##### preferred_model = Sequential(\n",
    "    [ \n",
    "        Dense(25, activation = 'relu'),\n",
    "        Dense(15, activation = 'relu'),\n",
    "        Dense(4, activation = 'linear')   #<-- Note\n",
    "    ]\n",
    ")\n",
    "##### preferred_model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),  #<-- Note\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now there are better optimization algorithms used instead of  gradient descent. One of them is ADAM-Adaptive Moment Estimation. It basically changes the learning rate during training. It is computationally efficient and is used to update the weights and biases of the neural network during training. It is an extension of the stochastic gradient descent algorithm. It is used to minimize the loss function during training to improve the performance of the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The layers we have been using are called dense layers. In dense layers each layer gets all the outputs of the previous layer as the input. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional layer: Convolutional layers are used to extract features from the input data. They are used in image recognition tasks. Convolutional layers are used to detect patterns in the input data. They are used to extract features from the input data. Convolutional layers are used in image recognition tasks. They take only a part of the output of the previous layer as the input thus making the training process computationally efficient. If we have multiple convolutional layers then such a neural network is called a Convolutional Neural Network(CNN). CNNs are used in image recognition tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging a learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example we implemented regularized linear regression and it is not working as expected. We can use the following steps to debug the learning algorithm:\n",
    "\n",
    "1. Get more training examples: If the training set is small then the model may not be able to learn the underlying pattern in the data. We can get more training examples to improve the performance of the model.\n",
    "\n",
    "2. Try a smaller set of features: If the model is overfitting the training data then we can try a smaller set of features to improve the performance of the model. We can remove the less important features from the model to improve the performance of the model.\n",
    "\n",
    "3. Try a larger set of features: If the model is underfitting the training data then we can try a larger set of features to improve the performance of the model. We can add more features to the model to improve the performance of the model.\n",
    "\n",
    "4. Try a different model: If the model is not performing well then we can try a different model to improve the performance of the model. We can try a different learning algorithm to improve the performance of the model.\n",
    "\n",
    "5. Try a different optimization algorithm: If the model is not converging then we can try a different optimization algorithm to improve the performance of the model. We can try a different optimization algorithm to improve the performance of the model.\n",
    "\n",
    "6. Try a different activation function: If the model is not converging then we can try a different activation function to improve the performance of the model. We can try a different activation function to improve the performance of the model.\n",
    "\n",
    "7. Try a different loss function: If the model is not converging then we can try a different loss function to improve the performance of the model. We can try a different loss function to improve the performance of the model.\n",
    "\n",
    "8. Try a different regularization parameter: If the model is overfitting the training data then we can try a different regularization parameter to improve the performance of the model. We can try a different regularization parameter to improve the performance of the model.\n",
    "\n",
    "9. Try a different learning rate: If the model is not converging then we can try a different learning rate to improve the performance of the model. We can try a different learning rate to improve the performance of the model.\n",
    "\n",
    "10. Try a different batch size: If the model is not converging then we can try a different batch size to improve the performance of the model. We can try a different batch size to improve the performance of the model.\n",
    "\n",
    "11. Try a different number of epochs: If the model is not converging then we can try a different number of epochs to improve the performance of the model. We can try a different number of epochs to improve the performance of the model.\n",
    "\n",
    "12. Try a different initialization method: If the model is not converging then we can try a different initialization method to improve the performance of the model. We can try a different initialization method to improve the performance of the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the performance of a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data into training and testing data. We train the model on the training data and evaluate the performance of the model on the testing data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For deciding which model to use we split the data into three subsets- training, validation and testing data. We train the model on the training data and evaluate the performance of the model on the validation data. We select the model with the best performance on the validation data. We then evaluate the performance of the selected model on the testing data. This is called the holdout method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
