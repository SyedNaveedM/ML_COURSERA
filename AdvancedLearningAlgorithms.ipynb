{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "##### Neural networks were built to mimic the functioning of the human brain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Traditional learning algorithms were not able to scale their performance with the amount of data available and reached a stagnation point. This is where neural networks come into play. They are able to learn from large amounts of data and generalize well on unseen data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For understanding the working of neural networks, we need to understand the basic building blocks of neural networks. Let us take a simple example of demand prediction for a product. Let x be the price of the product and y be the demand for the product. We can represent this relationship as y = f(x). The function f(x) is the neural network that we are trying to learn. Let us use logistic regression to classify the product as high demand or low demand. The logistic regression model can be represented as y = sigmoid(w*x + b). Here w and b are the weights and bias of the model. The sigmoid function is used to convert the output of the linear model to a probability value between 0 and 1. This is a single neuron neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now let us consider a more complex example where we have multiple features to predict the demand for the product. We can represent this as y = f(x1, x2, x3, ..., xn). The function f(x1, x2, x3, ..., xn) is the neural network that we are trying to learn. We can represent this as y = sigmoid(w1*x1 + w2*x2 + w3*x3 + ... + wn*xn + b). This is a multi-layer neural network with multiple neurons in the hidden layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A layer in a neural network is a collection of neurons that perform the same operation on the input data. The input layer is the first layer of the neural network that takes the input data. The hidden layers are the layers between the input and output layers. The output layer is the last layer of the neural network that produces the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Activation functions are used to introduce non-linearity in the neural network. The most commonly used activation functions are sigmoid, tanh, and ReLU. The sigmoid function is used to convert the output of the linear model to a probability value between 0 and 1. The tanh function is used to convert the output of the linear model to a value between -1 and 1. The ReLU function is used to introduce non-linearity in the neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ReLU is the most commonly used activation function in deep learning. It is defined as f(x) = max(0, x). ReLU is computationally efficient and does not suffer from the vanishing gradient problem. The vanishing gradient problem occurs when the gradient of the activation function becomes very small, leading to slow convergence of the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Each neuron in a particular layer has access to all the outputs of the previous layer and it eventually learns to ignore the less important features or activations. This is done by adjusting the weights and biases of the neurons during the training process. The weights and biases are updated using an optimization algorithm such as gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A neural network does the feature engineering which we had to do manually in traditional machine learning algorithms. It automatically learns the important features from the data and generalizes well on unseen data. This is the power of neural networks and deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Face recognition is done using neural networks. Basically any image is represented as a matrix of pixel values. The neural network learns the important features from the image and classifies it as a particular person. This is done by training the neural network on a large dataset of images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### By convention we use layer 0 for the input layer, layer 1 for the first hidden layer, layer 2 for the second hidden layer, and so on. The output layer is the last layer of the neural network. We use a superscript in square brackets to denote the layer number. For example, a[1] is the activation of the first hidden layer and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Activation value of layer l, unit neuron j is z[l][j] = g(w[l][j] . a[l-1] + b[l][j]) where g is the activation function. The activation value of layer l, unit neuron j is a[l][j] = g(z[l][j]). One example of g is the sigmoid function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward propogation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Forward propogation is used for making predictions from neural networks. It is the process of passing the input data through the neural network to get the output. The output of the neural network is the prediction made by the model. The forward propagation algorithm is as follows:\n",
    "\n",
    "##### 1. Initialize the input layer with the input data.\n",
    "##### 2. For each layer l from 1 to L:\n",
    "#####     a[l] = g(z[l])\n",
    "#####     z[l] = w[l] . a[l-1] + b[l]\n",
    "##### 3. The output of the neural network is the activation value of the output layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
