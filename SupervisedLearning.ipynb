{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Machine learning is the science of making computers learn without the need to be explicitly programmed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Supervised learning algorithms are the algorithms which learn from input to output mappings. Basically, they learn from examples where we give them the input and also we give the correct output from which they gradually learn and are able to make accurate predictions on any new inputs they encounter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ex:- Speech recognition, translation, online advertising, spam filter, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There are two types of supervised learning algorithms: classification and regression.\n",
    "##### Classification: In classification what we do is classify the input into one or more categories. Classification is basically discrete as there a finite number of outputs possible.\n",
    "##### Regression: In regression we produce a number or any other value which is continuous. It has infinite number of possible outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What the classification algorithms do is decide a boundary line for each category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In unsupervised learning we are not given any output labels for the input. Instead the unsupervised learning algorithm tries to find something interesting like a pattern in the data. This is different from supervised learning because we ourselves don't know what the right answer is for the patterns so we use unsupervised learning algorithms to figure that out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classifying the input into different groups or clusters is called clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Another type of unsupervised learning is anomaly detection which detects any outliers or unusual points in the data. This is widely used in fraud detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The most basic regression algorithm is linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This algorithm finds the best fit line for the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The dataset which is used to train the model is called the training set. All the column headers in the dataset are called features of a model and the final output that we desire is called the output or target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The function for univariate linear regression is f(x)=wx+b where x is the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In the function f(x)=wx+b, w and b are called parameters/coefficients/weights of the model. These are variables whose value we change while training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To determine how well our model fits the data or how well the model works we use the cost function. The cost function for linear regression is,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function \\( J(w, b) \\) for linear regression is given by:\n",
    "\n",
    "$$\n",
    "J(w, b) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( f(x_i) - y_i \\right)^2\n",
    "$$\n",
    "\n",
    "where:\n",
    "- \\( m \\) is the number of training examples.\n",
    "- \\( f(x_i) \\) is the predicted value for the \\( i \\)-th example.\n",
    "- \\( y_i \\) is the actual value for the \\( i \\)-th example.\n",
    "- \\( w \\) and \\( b \\) are the parameters of the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The cost function is the average of the squared differences between the predicted value and the actual value. The cost function is also called the mean squared error.\n",
    "\n",
    "##### The goal of the linear regression model is to minimize the cost function. This is done by changing the values of the parameters w and b."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Contour plot is a graphical representation of the cost function. It is a 3D plot where the x-axis and y-axis represent the values of the parameters w and b and the z-axis represents the cost function.\n",
    "\n",
    "##### The contour plot is used to visualize the cost function and to find the minimum value of the cost function.\n",
    "\n",
    "##### The gradient descent algorithm is used to minimize the cost function. The gradient descent algorithm is an optimization algorithm that is used to find the minimum value of the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The most common optimization algorithm used to minimize the cost function is the gradient descent algorithm.\n",
    "\n",
    "##### The gradient descent algorithm is an iterative optimization algorithm that is used to minimize the cost function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The gradient descent does not return the global minima instead it returns the local minima. The global minima is the point where the cost function is minimum. The local minima is the point where the cost function is minimum in a small region of the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent for Linear Regression (Without Cost Function)\n",
    "\n",
    "The goal of linear regression is to find the weights (or coefficients) that best fit the training data. Gradient descent is used to iteratively update the weights in order to minimize the error between predicted and actual values.\n",
    "\n",
    "## Gradient Descent Algorithm\n",
    "\n",
    "The weights are updated iteratively using the following rule:\n",
    "\n",
    "$$\n",
    "\\mathbf{w} := \\mathbf{w} - \\alpha \\nabla J(\\mathbf{w})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **w** is the vector of weights (parameters).\n",
    "- **α** is the learning rate, which controls the step size of each update.\n",
    "- **∇J(w)** is the gradient of the error with respect to the weights.\n",
    "\n",
    "### Gradient Calculation\n",
    "\n",
    "The gradient of the error with respect to the weights **w** is:\n",
    "\n",
    "$$\n",
    "\\nabla J(\\mathbf{w}) = \\frac{1}{m} \\sum_{i=1}^{m} \\left( \\hat{y}_i - y_i \\right) \\mathbf{x}_i\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **m** is the number of training examples.\n",
    "- **ŷᵢ** is the predicted value for the *i*-th example: \\( \\hat{y}_i = \\mathbf{x}_i^T \\mathbf{w} \\).\n",
    "- **yᵢ** is the actual value for the *i*-th example.\n",
    "- **xᵢ** is the feature vector for the *i*-th example.\n",
    "\n",
    "### Weight Update Rule\n",
    "\n",
    "Using the gradient, the weight update rule for each weight **wⱼ** is:\n",
    "\n",
    "$$\n",
    "w_j := w_j - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} \\left( \\hat{y}_i - y_i \\right) x_{ij}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **wⱼ** is the *j*-th weight.\n",
    "- **xᵢⱼ** is the *j*-th feature of the *i*-th training example.\n",
    "\n",
    "## Iterative Process\n",
    "\n",
    "Repeat the following steps until convergence or for a set number of iterations:\n",
    "1. Calculate the predictions **ŷᵢ** for each training example:\n",
    "   $$\n",
    "   \\hat{y}_i = \\mathbf{x}_i^T \\mathbf{w}\n",
    "   $$\n",
    "2. Update the weights using the gradient update rule:\n",
    "   $$\n",
    "   \\mathbf{w} := \\mathbf{w} - \\alpha \\nabla J(\\mathbf{w})\n",
    "   $$\n",
    "\n",
    "This algorithm ensures that the weights are adjusted to minimize the error in the predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In gradient descent we have to update the weights simultaneously. We can't update the weights one by one. We have to update all the weights at the same time. This is because if we update the weights one by one then the weights will not converge to the minimum value of the cost function. Basically when we update w then we have to use the new value of w to update b and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The efficiency of our gradient descent algorithm depends on the learning rate alpha too. If the learning rate is too small then the algorithm will take a long time to converge to the minimum value of the cost function. If the learning rate is too large then the algorithm may overshoot the minimum value of the cost function and may not converge to the minimum value of the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The above process is called batch gradient descent. In batch gradient descent we use all the training examples to update the weights. This is called batch gradient descent because we use all the training examples to update the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Linear Regression\n",
    "##### In multivariate linear regression we have more than one input feature. The function for multivariate linear regression is f(x)=w1x1+w2x2+...+wnxn+b where x1, x2, ..., xn are the input features.\n",
    "##### The cost function for multivariate linear regression is the same as the cost function for univariate linear regression. The only difference is that we have more than one input feature.\n",
    "##### The gradient of the cost function for multivariate linear regression is the same as the gradient of the cost function for univariate linear regression. The only difference is that we have more than one input feature.\n",
    "##### The weight update rule for multivariate linear regression is the same as the weight update rule for univariate linear regression. The only difference is that we have more than one input feature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function for multivariate linear regression is f(x)=w1x1+w2x2+...+wnxn+b where x1, x2, ..., xn are the input features.\n",
    "##### f(x)=w.x+b where x is the input feature vector and w is the weight vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Implementing the function without vectorization\n",
    "```python\n",
    "f = 0\n",
    "for i in range(n):\n",
    "    f += w[i] * x[i]\n",
    "f += b\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Implementing the function with vectorization\n",
    "```python\n",
    "f = np.dot(w, x) + b\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature scaling\n",
    "##### Feature scaling is a technique used to standardize the range of independent variables or features of data. In data processing, it is also known as data normalization and is generally performed during the data preprocessing step.\n",
    "##### Feature scaling is important because it helps to normalize the data within a particular range. This is important because if the data is not normalized then the weights will not converge to the minimum value of the cost function.\n",
    "##### There are two common ways to scale the features: min-max scaling and standardization.\n",
    "##### Min-max scaling scales the data to a fixed range, usually 0 to 1.\n",
    "##### Standardization scales the data so that it has a mean of 0 and a standard deviation of 1.\n",
    "##### Min-max scaling is useful when the data has a normal distribution. Standardization is useful when the data has a non-normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Z-score normalization is a method of normalizing data. It is also known as standardization. Z-score normalization scales the data so that it has a mean of 0 and a standard deviation of 1. Z-score normalization is useful when the data has a non-normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking if our gradient descent algorithm is working correctly\n",
    "##### To check if our gradient descent algorithm is working correctly we can plot the cost function vs the number of iterations. If the cost function is decreasing with each iteration then our gradient descent algorithm is working correctly. If the cost function is increasing with each iteration then our gradient descent algorithm is not working correctly.\n",
    "##### We can also plot the cost function vs the weights. If the cost function is decreasing with each iteration then our gradient descent algorithm is working correctly. If the cost function is increasing with each iteration then our gradient descent algorithm is not working correctly. This plotting is called the convergence plot/learing curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Automatic convergence test: Let the cost function be J(w, b). If J(w, b) decreases by less than a certain threshold value in one iteration then we can say that the algorithm has converged. This is called the automatic convergence test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the learning rate(alpha) correctly\n",
    "##### The learning rate is a hyperparameter that controls the step size of each update. If the learning rate is too small then the algorithm will take a long time to converge to the minimum value of the cost function. If the learning rate is too large then the algorithm may overshoot the minimum value of the cost function and may not converge to the minimum value of the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering\n",
    "##### Feature engineering is the process of using domain knowledge to extract features from raw data\n",
    "##### We use feature engineering to create new features from the existing features. This is done to improve the performance of the model.\n",
    "##### Feature engineering is important because it helps to improve the performance of the model. It helps to create new features from the existing features. This is done to improve the performance of the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial regression\n",
    "##### Polynomial regression is a type of regression analysis in which the relationship between the independent variable x and the dependent variable y is modeled as an nth degree polynomial.\n",
    "##### Polynomial regression is used when the relationship between the independent variable x and the dependent variable y is not linear.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
