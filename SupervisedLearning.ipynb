{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Machine learning is the science of making computers learn without the need to be explicitly programmed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Supervised learning algorithms are the algorithms which learn from input to output mappings. Basically, they learn from examples where we give them the input and also we give the correct output from which they gradually and are able to make accurate predictions on any new inputs they encounter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ex:- Speech recognition, translation, online advertising, spam filter, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There are two types of supervised learning algorithms: classification and regression.\n",
    "##### Classification: In classification what we do is classify the input into one or more categories. Classification is basically discrete as there a finite number of outputs possible.\n",
    "##### Regression: In regression we produce a number or any other value which is continuous. It has infinite number of possible outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What the classification algorithms do is decide a boundary line for each category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In unsupervised learning we are not given any output labels for the input. Instead the unsupervised learning algorithm tries to find something interesting like a pattern in the data. This is different supervised learning because we ourselves don't know what the right answer is for the patterns so we use unsupervised learning algorithms to figure that out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classifying the input into different groups or clusters is called clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Another type of unsupervised learning is anomaly detection which detects any outliers or unusual points in the data. This is widely used in fraud detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The most basic regression algorithm is linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This algorithm finds the best fit line for the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The dataset which is used to train the model is called the training set. All the column headers in the dataset are called features of a model and the final output that we desire is called the output or target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The function for univariate linear regression is f(x)=wx+b where x is the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In the function f(x)=wx+b, w and b are called parameters/coefficients/weights of the model. These are variables whose value we change while training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To determine how well our model fits the data or how well the model works we use the cost function. The cost function for linear regression is,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function \\( J(w, b) \\) for linear regression is given by:\n",
    "\n",
    "$$\n",
    "J(w, b) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( f(x_i) - y_i \\right)^2\n",
    "$$\n",
    "\n",
    "where:\n",
    "- \\( m \\) is the number of training examples.\n",
    "- \\( f(x_i) \\) is the predicted value for the \\( i \\)-th example.\n",
    "- \\( y_i \\) is the actual value for the \\( i \\)-th example.\n",
    "- \\( w \\) and \\( b \\) are the parameters of the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The cost function is the average of the squared differences between the predicted value and the actual value. The cost function is also called the mean squared error.\n",
    "\n",
    "##### The goal of the linear regression model is to minimize the cost function. This is done by changing the values of the parameters w and b."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Contour plot is a graphical representation of the cost function. It is a 3D plot where the x-axis and y-axis represent the values of the parameters w and b and the z-axis represents the cost function.\n",
    "\n",
    "##### The contour plot is used to visualize the cost function and to find the minimum value of the cost function.\n",
    "\n",
    "##### The gradient descent algorithm is used to minimize the cost function. The gradient descent algorithm is an optimization algorithm that is used to find the minimum value of the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The most common optimization algorithm used to minimize the cost function is the gradient descent algorithm.\n",
    "\n",
    "##### The gradient descent algorithm is an iterative optimization algorithm that is used to minimize the cost function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The gradient descent algorithm is used to minimize the cost function. The gradient descent algorithm is an optimization algorithm that is used to find the minimum value of the cost function.\n",
    "\n",
    "##### Gradient descent is a general algorithm that can be used to minimize any function. The gradient descent algorithm is used to minimize the cost function by changing the values of the parameters w and b.\n",
    "\n",
    "##### It works like this:\n",
    "##### 1. Initialize the parameters w and b to some random values.\n",
    "##### 2. Calculate the gradient of the cost function with respect to the parameters w and b.\n",
    "##### 3. Update the parameters w and b using the gradient of the cost function.\n",
    "##### 4. Repeat steps 2 and 3 until the cost function converges to a minimum value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The gradient descent does not return the global minima instead it returns the local minima. The global minima is the point where the cost function is minimum. The local minima is the point where the cost function is minimum in a small region of the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent for Linear Regression (Without Cost Function)\n",
    "\n",
    "The goal of linear regression is to find the weights (or coefficients) that best fit the training data. Gradient descent is used to iteratively update the weights in order to minimize the error between predicted and actual values.\n",
    "\n",
    "## Gradient Descent Algorithm\n",
    "\n",
    "The weights are updated iteratively using the following rule:\n",
    "\n",
    "$$\n",
    "\\mathbf{w} := \\mathbf{w} - \\alpha \\nabla J(\\mathbf{w})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **w** is the vector of weights (parameters).\n",
    "- **α** is the learning rate, which controls the step size of each update.\n",
    "- **∇J(w)** is the gradient of the error with respect to the weights.\n",
    "\n",
    "### Gradient Calculation\n",
    "\n",
    "The gradient of the error with respect to the weights **w** is:\n",
    "\n",
    "$$\n",
    "\\nabla J(\\mathbf{w}) = \\frac{1}{m} \\sum_{i=1}^{m} \\left( \\hat{y}_i - y_i \\right) \\mathbf{x}_i\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **m** is the number of training examples.\n",
    "- **ŷᵢ** is the predicted value for the *i*-th example: \\( \\hat{y}_i = \\mathbf{x}_i^T \\mathbf{w} \\).\n",
    "- **yᵢ** is the actual value for the *i*-th example.\n",
    "- **xᵢ** is the feature vector for the *i*-th example.\n",
    "\n",
    "### Weight Update Rule\n",
    "\n",
    "Using the gradient, the weight update rule for each weight **wⱼ** is:\n",
    "\n",
    "$$\n",
    "w_j := w_j - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} \\left( \\hat{y}_i - y_i \\right) x_{ij}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **wⱼ** is the *j*-th weight.\n",
    "- **xᵢⱼ** is the *j*-th feature of the *i*-th training example.\n",
    "\n",
    "## Iterative Process\n",
    "\n",
    "Repeat the following steps until convergence or for a set number of iterations:\n",
    "1. Calculate the predictions **ŷᵢ** for each training example:\n",
    "   $$\n",
    "   \\hat{y}_i = \\mathbf{x}_i^T \\mathbf{w}\n",
    "   $$\n",
    "2. Update the weights using the gradient update rule:\n",
    "   $$\n",
    "   \\mathbf{w} := \\mathbf{w} - \\alpha \\nabla J(\\mathbf{w})\n",
    "   $$\n",
    "\n",
    "This algorithm ensures that the weights are adjusted to minimize the error in the predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In gradient descent we have to update the weights simultaneously. We can't update the weights one by one. We have to update all the weights at the same time. This is because if we update the weights one by one then the weights will not converge to the minimum value of the cost function. Basically when we update w then we have to use the new value of w to update b and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The efficiency of our gradient descent algorithm depends on the learning rate alpha too. If the learning rate is too small then the algorithm will take a long time to converge to the minimum value of the cost function. If the learning rate is too large then the algorithm may overshoot the minimum value of the cost function and may not converge to the minimum value of the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The above process is called batch gradient descent. In batch gradient descent we use all the training examples to update the weights. This is called batch gradient descent because we use all the training examples to update the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
